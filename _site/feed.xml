<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://blog.myola.se/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.0">Jekyll</generator><link href="http://blog.myola.se/feed.xml" rel="self" type="application/atom+xml" /><link href="http://blog.myola.se/" rel="alternate" type="text/html" /><updated>2016-11-17T02:13:34+01:00</updated><id>http://blog.myola.se//</id><title type="html">Ola Theander’s blog</title><subtitle>Software Engineer with special interest in Distributed Computing and Cloud Technology</subtitle><entry><title type="html">Provisioning DC/OS on AWS using Ansible</title><link href="http://blog.myola.se/Ansible-DCOS-AWS/" rel="alternate" type="text/html" title="Provisioning DC/OS on AWS using Ansible" /><published>2016-11-16T00:00:00+01:00</published><updated>2016-11-16T00:00:00+01:00</updated><id>http://blog.myola.se/Ansible-DCOS-AWS</id><content type="html" xml:base="http://blog.myola.se/Ansible-DCOS-AWS/">&lt;p&gt;In this first post I’ll present my current pet project for provisioning DC/OS on Amazon AWS using Ansible called &lt;a href=&quot;https://github.com/olatheander/ansible-dcos-aws-playbook&quot;&gt;ansible-dcos-aws-playbook&lt;/a&gt; (hosted on GitHub). I’m really interested in the technology and ideas that emerged in recent years and that is rapidly becoming mainstream, i.e. containers (Docker), container orchestration (Mesos, DC/OS, Kubernetes etc.), cloud native applications and infrastructure-as-code. There are quite many ways that you can experiment with this technology, it’s a breeze configuring a basic DC/OS cluster on AWS using a CloudFormation template and is equally simple on Azure. This is all good but I don’t quite fancy the wizard approach even if its admittedly handy.&lt;/p&gt;

&lt;p&gt;What I would like to have is a way to automatically and consistently setup and tear down a cluster over and over, according to &lt;em&gt;immutable infrastructure&lt;/em&gt; and &lt;em&gt;infrastructure-as-code&lt;/em&gt; principles. This gives the freedom to experiment and if something is messed up, just tear down the cluster and build it up again and you will have a fresh start. By having the cluster setup defined by code, in this case an Ansible playbook, the cluster configuration can be debugged and improved over time, just like any software code. Doing stuff manually doesn’t quite provide the same consistency.&lt;/p&gt;

&lt;p&gt;Secondly, I would also like to have a provider-agnostic way of configuring the cluster. Nothing wrong with the wizard approach but it kind of couples you to a specific provider. Switching to a different provider would include a learning curve and possibly technical difficulties. The approach taken by the &lt;em&gt;ansible-dcos-aws-playbook&lt;/em&gt; project is instead to rely on very basic IaaS services such as virtual machines and networking, potentially configuring the cluster on any IaaS provider or in a private data center.&lt;/p&gt;

&lt;p&gt;The ambition is that the configured cluster should be a production worthy setup according to best practices (its not quite there yet but it is progressing) but it can also be used to experiment with different technology stacks and/or different configurations.&lt;/p&gt;

&lt;p&gt;The current version is developed and tested on AWS but the plan is to support GCE and Azure as well as on-premise metal or VM cloud. Likely, this will result in spin-off sibling projects such as ansible-dcos-azure-playbook that manage the IaaS infrastructure while reusing the DC/OS related roles. A similar project is also in the pipe for Kubernetes.&lt;/p&gt;

&lt;h1 id=&quot;try-it-out&quot;&gt;Try it out&lt;/h1&gt;
&lt;p&gt;Enough background, the rest of the post will guide you through a tryout of the project. In the end you will, if all goes well, have a working DC/OS cluster provisioned on an AWS virtual private cloud (VPC) fronted by an elastic load-balancer (ELB) and a simple demo application running.&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;To try the project, you’ll need:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;python, python-pip, boto and &lt;a href=&quot;https://pypi.python.org/pypi/boto3&quot;&gt;boto3&lt;/a&gt; installed.&lt;/li&gt;
  &lt;li&gt;Ansible 2.2 or later &lt;a href=&quot;http://docs.ansible.com/ansible/intro_installation.html&quot;&gt;installed&lt;/a&gt; — the cluster is configuration is defined by an Ansible &lt;a href=&quot;http://docs.ansible.com/ansible/playbooks.html&quot;&gt;playbook&lt;/a&gt;. Ansible is my tool of choice since it quite powerful and agent-less. The only requirement is that the target host(s) support SSH and have Python installed. Version 2.2 is required since it comes with some new and nice updated cloud modules, especially related to networking.&lt;/li&gt;
  &lt;li&gt;An Amazon AWS account.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Configure your AWS credentials in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.boto&lt;/code&gt; as described in &lt;a href=&quot;http://boto.readthedocs.io/en/latest/boto_config_tut.html&quot;&gt;Boto Config&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Credentials]
aws_access_key_id = &amp;lt;your_access_key_here&amp;gt;
aws_secret_access_key = &amp;lt;your_secret_key_here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Add your AWS SSH key to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh-agent&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh-agent bash 
$ ssh-add ~/.ssh/&amp;lt;amazon key file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is needed for the Ansible playbook to properly authenticate towards AWS and the EC2 instances. The &lt;code class=&quot;highlighter-rouge&quot;&gt;amazon key file&lt;/code&gt; should be the private part of the key you specify below for &lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_EC2_KEY_PAIR&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;
&lt;p&gt;Begin by cloning the repository&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git clone https://github.com/olatheander/ansible-dcos-aws-playbook.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Initialize the Git submodules (the generic Ansible roles are included in the playbook as submodules)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git submodule update --recursive --init
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Configure the AWS environment variables:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ export AWS_EC2_KEY_PAIR = &amp;lt;the name of the key-pair for EC2 instances&amp;gt;
$ export AWS_EC2_LB_CERT = &amp;lt;the arn resource for the AWS ELB certificate&amp;gt;
$ export AWS_S3_ELB_LOGS_BUCKET_NAME = &amp;lt;the name of the S3 bucket where to store the ELB logs&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By defining these two variables a simple hello-world demo is installed on the cluster that we will later experiment with.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ export DCOS_CONTROLLER_INSTALL_DEMOS = true
$ export DCOS_CONTROLLER_INSTALL_DOCKERCLOUD_HELLO_WORLD = true
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_EC2_KEY_PAIR&lt;/code&gt; is one of your keys found in &lt;em&gt;EC2 -&amp;gt; Network &amp;amp; Security -&amp;gt; KeyPairs&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_EC2_LB_CERT&lt;/code&gt; is ARN identifier found in the AWS Certificate Manager for the cert.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_S3_ELB_LOGS_BUCKET_NAME&lt;/code&gt; is the bucket id found by selecting &lt;em&gt;Properties&lt;/em&gt; for the bucket in S3.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;DCOS_CONTROLLER_INSTALL_DEMOS&lt;/code&gt; variable is a generic switch for installing demo(s). When false this section is skipped altogether.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;DCOS_CONTROLLER_INSTALL_DOCKERCLOUD_HELLO_WORLD&lt;/code&gt; toggles the installation of the &lt;a href=&quot;https://github.com/docker/dockercloud-hello-world&quot;&gt;Dockercloud Hello World&lt;/a&gt; Docker container demo.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;project-layout&quot;&gt;Project layout&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
|-- community
|   `-- roles
|       |-- dcos
|       |-- dcos-bootstrap
|       |-- dcos-controller
|       |-- docker
|       `-- pip
|-- filter_plugins
|-- group_vars
`-- roles
    `-- aws
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you have some previous experience of Ansible this is probably old hat (read more about &lt;a href=&quot;http://docs.ansible.com/ansible/playbooks_best_practices.html&quot;&gt;best-practices&lt;/a&gt;) but worth mentioning is the reusable roles included that are all stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;./community/roles&lt;/code&gt;
 as Git submodules. This is also the option of using &lt;a href=&quot;https://galaxy.ansible.com/&quot;&gt;Ansible Galaxy&lt;/a&gt;, recently open-sourced, but I prefer the submodule approach.&lt;/p&gt;

&lt;h2 id=&quot;run-playbooks-to-setup-the-cluster&quot;&gt;Run playbooks to setup the cluster&lt;/h2&gt;
&lt;p&gt;Setting up the cluster consists of running two playbooks, first:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ansible-playbook -i ec2.py main.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This playbook configures the AWS infrastructure by allocating EC2 instances, setting up VPC, load-balancer, network gateways etc. Running the playbook, you will see a lot of tasks scrolling by and it will take a few minutes to complete. When done there should be no errors and a message like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TASK [debug] *******************************************************************
ok: [localhost] =&amp;gt; {
    &quot;msg&quot;: &quot;*** Playbook done! ***&quot;
}

PLAY RECAP *********************************************************************
localhost                  : ok=18   changed=3    unreachable=0    failed=0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If something goes wrong Ansible playbooks are idempotent so you can just fix the problem and re-run it.&lt;/p&gt;

&lt;p&gt;When completed, check your AWS VPC dashboard and there is a VPC configured named &lt;em&gt;dcos-cluster-vpc&lt;/em&gt; with 10 EC2 instances running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/EC2 instances in VPC.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note, dont keep the cluster running for long&lt;/strong&gt; if you’re just experimenting. Tear down the cluster when done, see end of this article for instructions. For DCOS to be able to schedule containers onto the nodes its better to have fewer big nodes rather than many small since the scheduler may not be able to find a node with enough free resources if nodes are too small. The downside is that this node type is pretty expensive.&lt;/p&gt;

&lt;p&gt;Next step is to provision the allocated resources i.e. setting up a DCOS cluster on top. Do this by running the playbook &lt;code class=&quot;highlighter-rouge&quot;&gt;provision_cluster.yml&lt;/code&gt;. This playbook is basically a straightforward implementation of the &lt;a href=&quot;https://dcos.io/docs/1.9/administration/installing/custom/advanced/&quot;&gt;Advanced DC/OS Installation Guide&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ansible-playbook -i ec2.py provision_cluster.yml -u ubuntu
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;-u&lt;/code&gt; switch tells Ansible which user to use when authenticating towards the EC2 instances. By default, the AMI (&lt;code class=&quot;highlighter-rouge&quot;&gt;ami-c06b1eb3&lt;/code&gt;) provisioned is &lt;em&gt;Ubuntu Xenial 16.04&lt;/em&gt; but &lt;em&gt;CentOS&lt;/em&gt; and eventually &lt;em&gt;CoreOS&lt;/em&gt; will also be supported. &lt;code class=&quot;highlighter-rouge&quot;&gt;-i&lt;/code&gt; is the inventory parameter and &lt;code class=&quot;highlighter-rouge&quot;&gt;ec2.py&lt;/code&gt; is the &lt;a href=&quot;http://docs.ansible.com/ansible/intro_dynamic_inventory.html&quot;&gt;dynamic inventory&lt;/a&gt; script for AWS.&lt;/p&gt;

&lt;p&gt;When running you might face an error like below and the reason is most likely that the instances are not fully initialized yet. If this happens cancel the playbook and retry. When all nodes can be reached, let the playbook run to completion. I plan to fix this bug in a later version of the playbook by waiting until all nodes respond to SSH connections.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TASK [raw] *********************************************************************
fatal: [10.0.1.217]: UNREACHABLE! =&amp;gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to connect to the host via ssh: Connection timed out during banner exchange\r\n&quot;, &quot;unreachable&quot;: true}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There will also be a couple of ignored error related to python-pip but this is normal and just the role checking if pip is installed.&lt;/p&gt;

&lt;p&gt;It will take a couple of minutes setting up the cluster. Once the playbook is done you will see output like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TASK [dcos-controller : Launch external Dockercloud hello-world sample application.] ***
changed: [10.0.0.250]

PLAY RECAP *********************************************************************
10.0.0.173                 : ok=30   changed=20   unreachable=0    failed=0
10.0.0.250                 : ok=10   changed=8    unreachable=0    failed=0
10.0.0.42                  : ok=30   changed=20   unreachable=0    failed=0
10.0.1.122                 : ok=30   changed=20   unreachable=0    failed=0
10.0.1.123                 : ok=30   changed=20   unreachable=0    failed=0
10.0.1.124                 : ok=30   changed=20   unreachable=0    failed=0
10.0.1.215                 : ok=30   changed=20   unreachable=0    failed=0
10.0.1.216                 : ok=30   changed=20   unreachable=0    failed=0
10.0.1.217                 : ok=30   changed=20   unreachable=0    failed=0
10.0.2.211                 : ok=30   changed=20   unreachable=0    failed=0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;play-with-the-cluster&quot;&gt;Play with the cluster&lt;/h2&gt;

&lt;p&gt;Now that the cluster is up let’s play with it for a bit. By defining the environment variables &lt;code class=&quot;highlighter-rouge&quot;&gt;DCOS_CONTROLLER_INSTALL_DEMOS&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;DCOS_CONTROLLER_INSTALL_DOCKERCLOUD_HELLO_WORLD&lt;/code&gt; the cluster is already up and running with the &lt;a href=&quot;https://github.com/mesosphere/marathon-lb&quot;&gt;Marathon load-balancer&lt;/a&gt; and a simple Hello-world application that we can use to get to know the setup a bit better.&lt;/p&gt;

&lt;p&gt;First let’s check the AWS ELB status since it may take some time for it to detect the nodes being up and running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/ELB Instances.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see both DCOS public nodes are in-service. Switch to the &lt;em&gt;Description&lt;/em&gt; tab and copy the DNS name of the ELB, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;dcos-cluster-elb-2100053169.eu-west-1.elb.amazonaws.com&lt;/code&gt; and paste it in your browser. You should then be greeted by the page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/Dockercloud Hello-world.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;log-in-on-the-dcos-admin&quot;&gt;Log in on the DCOS admin&lt;/h3&gt;

&lt;p&gt;To be able to do some interesting stuff we need to be able to access the DCOS admin console and thats done by opening a SSH tunnel via the &lt;em&gt;dcos-ssh-jumphost&lt;/em&gt; instance as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh -i &amp;lt;path-to-ssh-key&amp;gt; -L 8443:&amp;lt;master-private-dns&amp;gt;:443 ubuntu@&amp;lt;public-dns-of-ssh-jumphost&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;path-to-ssh-key&lt;/code&gt; — your private key used above for provisioning the cluster nodes. The &lt;code class=&quot;highlighter-rouge&quot;&gt;-i&lt;/code&gt; switch is not needed if you’re setting up the tunnel from your &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh-agent&lt;/code&gt; shell.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;master-private-dns&lt;/code&gt; — the private DNS name of one of the &lt;em&gt;dcos-master&lt;/em&gt; nodes, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;ip-10-0-1-217.eu-west-1.compute.internal&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;public-dns-of-ssh-jumphost&lt;/code&gt; — the public DNS name of the ssh-jumphost node, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;ec2-52-213-150-225.eu-west-1.compute.amazonaws.com&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the tunnel is up you can now access the admin console at &lt;a href=&quot;https://localhost:8443&quot;&gt;https://localhost:8443&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/DCOS admin console.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click on the &lt;em&gt;docker-cloud-hello-world&lt;/em&gt; service and you’ll see on the detail view that we have a single instance of the application running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/dockercloud-hello-world service.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s try and scale it. Press the &lt;em&gt;Scale&lt;/em&gt; button in the upper right corner and scale it to three instances. After a few seconds there should now be three instances running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/hello-world scaled to three.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Refresh the web-page a couple of times and you will see that its automatically load-balanced in a round-robin fashion (notice the hostname changing):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/2016-11-16-Ansible-DCOS-AWS/hello-world host name.png&quot; alt=&quot;_config.yml&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scale it back down to one instance and you will have only one hostname. Pretty awesome I think.&lt;/p&gt;

&lt;h1 id=&quot;wrap-up&quot;&gt;Wrap up&lt;/h1&gt;
&lt;p&gt;In this post a DCOS cluster was configured with a simple application deployed but the long-term plan is to use the cluster as a foundation for more complex demos involving backbone services such as Kafka, databases, CI/CD pipelines, surveillance and monitoring etc.&lt;/p&gt;

&lt;p&gt;By having the demo’s setup defined as code its easy to extend with new demos thus providing a simple way to evaluate technology stacks.&lt;/p&gt;

&lt;p&gt;The long term goal is that the project should evolve into a scaffolding tool i.e. the cluster equivalent of &lt;a href=&quot;http://yeoman.io/&quot;&gt;Yeoman&lt;/a&gt; or &lt;a href=&quot;https://jhipster.github.io/&quot;&gt;JHipster&lt;/a&gt;, where you can pick and choose components to build a skeleton infrastructure based e.g. on the Netflix stack.&lt;/p&gt;

&lt;p&gt;It’s probably clear by now that the project is work in progress and if you like to contribute I’ll happily accept ideas, feedback, suggestions and/or pull-requests. Also, this post is a bit incomplete, among the things I would like to add is a schematic view of the cluster from a network perspective but as you know “Perfect is the enemy of good” so the post may be updated at a later date.&lt;/p&gt;

&lt;h1 id=&quot;tear-down-the-cluster&quot;&gt;Tear down the cluster&lt;/h1&gt;
&lt;p&gt;When you’re done experimenting with the cluster I recommend that you tear it down since its quite expensive to have it running for any longer period of time. If you want to play with it some more at some later time, just set it up again.&lt;/p&gt;

&lt;p&gt;At this point you have to tear down the cluster manually but I plan to offer a playbook to automate the process.&lt;/p&gt;

&lt;p&gt;To tear down the cluster do:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Terminate&lt;/em&gt; all EC2 cluster nodes using the AWS console (EC2 dashboard).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Delete&lt;/em&gt; the EC2 elastic load-balancer (EC2 -&amp;gt; Load Balancing -&amp;gt; Load Balancers).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Delete NAT Gateway&lt;/em&gt;, the NAT gateway take some time to delete but you have to wait for it to be in “Deleted” state otherwise it will lock resources preventing you from deleting the VPC (VPC Dashboard -&amp;gt; NAT Gateways).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Delete VPC&lt;/em&gt;, deleting the VPC will delete most of the resources still allocated (VPC Dashboard -&amp;gt; Your VPCs) except the EIP.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Release EIP&lt;/em&gt;, release the Elastic IP address. (VPC Dashboard -&amp;gt; Elastic IPs).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you now check the VPC dashboard you should have no resources left allocated (there might be a few resources like the NAT Gateway not yet garbage collected).&lt;/p&gt;</content><summary type="html">In this first post I’ll present my current pet project for provisioning DC/OS on Amazon AWS using Ansible called ansible-dcos-aws-playbook (hosted on GitHub). I’m really interested in the technology and ideas that emerged in recent years and that is rapidly becoming mainstream, i.e. containers (Docker), container orchestration (Mesos, DC/OS, Kubernetes etc.), cloud native applications and infrastructure-as-code. There are quite many ways that you can experiment with this technology, it’s a breeze configuring a basic DC/OS cluster on AWS using a CloudFormation template and is equally simple on Azure. This is all good but I don’t quite fancy the wizard approach even if its admittedly handy.</summary></entry></feed>
